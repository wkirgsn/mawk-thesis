% This file was created with JabRef 2.10b2.
% Encoding: Cp1252


@Misc{Tensorflow2015,
  Title                    = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},

  Author                   = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  Note                     = {Software available from tensorflow.org},
  Year                     = {2015},

  Url                      = {http://tensorflow.org/}
}

@Misc{Boecker2016,
  Title                    = {Controlled Three Phase Drives},

  Author                   = {Joachim B{\"{o}}cker},
  HowPublished             = {Lecture Notes},
  Year                     = {2016},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Grundlagen PMSM/Geregelte_Drehstromantriebe_Skript_DE.pdf:PDF},
  Review                   = {Vorlesung}
}

@Article{BaRa2015,
  Title                    = {Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning},
  Author                   = {Soheil Bahrampour and Naveen Ramakrishnan and Lukas Schott and Mohak Shah},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1511.06435},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/BahrampourRSS15},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1511.06435v3.pdf:PDF},
  Timestamp                = {Tue, 01 Dec 2015 19:22:34 +0100}
}

@Misc{Theano2012,
  Title                    = {Theano: new features and speed improvements},

  Author                   = {Bastien, Fr{\'e}d{\'e}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
  HowPublished             = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop},
  Year                     = {2012}
}

@Article{Bengio2009,
  Title                    = {Learning deep architectures for AI},
  Author                   = {Bengio, Yoshua},
  Journal                  = {Foundations and trends in Machine Learning},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {1--127},
  Volume                   = {2},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/ftml.pdf:PDF},
  Publisher                = {Now Publishers Inc.},
  Review                   = {Deep architectures are essential in order to learn certain tasks}
}

@InProceedings{BeBou2013,
  Title                    = {Advances in optimizing recurrent networks},
  Author                   = {Bengio, Yoshua and Boulanger-Lewandowski, Nicolas and Pascanu, Razvan},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {8624--8628},

  Comment                  = {http://arxiv.org/pdf/1212.0901.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1212.0901.pdf:PDF},
  Review                   = {Some tips for training RNNs}
}

@Article{BeSi1994,
  Title                    = {Learning long-term dependencies with gradient descent is difficult},
  Author                   = {Bengio, Y. and Simard, P. and Frasconi, P.},
  Journal                  = {Neural Networks, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {157-166},
  Volume                   = {5},

  Abstract                 = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\00279181.pdf:PDF},
  Keywords                 = {learning (artificial intelligence);numerical analysis;recurrent neural nets;efficient learning;gradient descent;input/output sequence mapping;long-term dependencies;prediction problems;production problems;recognition;recurrent neural network training;temporal contingencies;Computer networks;Cost function;Delay effects;Discrete transforms;Displays;Intelligent networks;Neural networks;Neurofeedback;Production;Recurrent neural networks},
  Review                   = {This paper shows why RNNs without LSTMs struggle learning long time dependencies. Storing information that is resistant to noise makes the gradient w.r.t. past events rapidly small relative to the gradient w.r.t. to recent events -> Vanishing Gradient. Not using LSTMs, the gradient descent of a cost function will vanish for robust classification if it requires long-term context, making gradient descent inefficient.}
}

@Article{BeBe2012,
  Title                    = {Random Search for Hyper-parameter Optimization},
  Author                   = {Bergstra, James and Bengio, Yoshua},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2012},

  Month                    = {2},
  Number                   = {1},
  Pages                    = {281--305},
  Volume                   = {13},

  Acmid                    = {2188395},
  Comment                  = {http://dl.acm.org/citation.cfm?id=2503308.2188395},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/p281-bergstra.pdf:PDF},
  Issue_date               = {January 2012},
  Keywords                 = {deep learning, global optimization, model selection, neural networks, response surface modeling},
  Numpages                 = {25},
  Publisher                = {JMLR.org},
  Review                   = {Introduces Random Search (for Hyperparameters). Random search slightly less efficient in low-dim search spaces than grid and manual search, but far more efficient for high-dim search spaces. That is due to the search space often having a low "effective dimension" i.e. some hyperparameters matter more than others. They propose an averaging over multiple best val-set performer to reduce uncertainty about best model. Plenty of suggestions for initializing hyperparameters.}
}

@Book{Bishop2006,
  Title                    = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  Author                   = {Bishop, Christopher M.},
  Publisher                = {Springer-Verlag New York, Inc.},
  Year                     = {2006},

  Address                  = {Secaucus, NJ, USA},

  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/Bishop - Pattern Recognition and Machine Learning.pdf:PDF},
  ISBN                     = {0387310738},
  Review                   = {Pattern Recognition basics}
}

@Article{BoCa2009,
  Title                    = {Evolution and Modern Approaches for Thermal Analysis of Electrical Machines},
  Author                   = {A. Boglietti and A. Cavagnino and D. Staton and M. Shanel and M. Mueller and C. Mejuto},
  Journal                  = {IEEE Transactions on Industrial Electronics},
  Year                     = {2009},

  Month                    = {3},
  Number                   = {3},
  Pages                    = {871-882},
  Volume                   = {56},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/ModernThermalAnalysis.pdf:PDF},
  Keywords                 = {computational fluid dynamics;electric motors;finite element analysis;lumped parameter networks;computational fluid dynamics;electric motor;electrical machines;finite-element analysis;lumped-parameter thermal network;thermal analysis;thermal parameter determination;Books;Circuits;Computational fluid dynamics;Electromagnetic analysis;Finite element methods;Resistance heating;Surface resistance;Thermal conductivity;Thermal engineering;Thermal resistance;Computed fluid dynamic;electrical machines;finite-element analysis (FEA);lumped-parameter thermal network (LPTN);thermal model;thermal parameter identification}
}

@Article{CaTa2010,
  Title                    = {On over-fitting in model selection and subsequent selection bias in performance evaluation},
  Author                   = {Cawley, Gavin C and Talbot, Nicola LC},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {2079--2107},
  Volume                   = {11},

  Comment                  = {http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Systemidentification/cawley10a.pdf:PDF},
  Publisher                = {JMLR. org}
}

@Article{ChoMe2014,
  Title                    = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  Author                   = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1406.1078},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChoMGBSB14},
  Comment                  = {http://arxiv.org/abs/1406.1078},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1406.1078v3.pdf:PDF},
  Review                   = {GRU introduced. Combines forget and input gates into single update gate. Merges cell state and hidden state and more.}
}

@Article{ChoBa2015,
  Title                    = {Attention-Based Models for Speech Recognition},
  Author                   = {Jan Chorowski and Dzmitry Bahdanau and Dmitriy Serdyuk and KyungHyun Cho and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.07503},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChorowskiBSCB15},
  Comment                  = {http://arxiv.org/abs/1506.07503},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1506.07503.pdf:PDF},
  Review                   = {Attention-RNN for Acoustic Modeling Has References to other attention based applications!},
  Timestamp                = {Wed, 01 Jul 2015 15:10:24 +0200}
}

@Article{ChuCa2015,
  Title                    = {Gated Feedback Recurrent Neural Networks},
  Author                   = {Junyoung Chung and {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and KyungHyun Cho and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1502.02367},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChungGCB15},
  Comment                  = {http://arxiv.org/abs/1502.02367},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1502.02367.pdf:PDF},
  Review                   = {Gated Feedback RNNs (GFRNN) introduced. Similar to CW-RNN but implements GRU/LSTM and is a fully connected stacked RNN},
  Timestamp                = {Mon, 02 Mar 2015 14:17:34 +0100}
}

@Article{ChuGu2014,
  Title                    = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  Author                   = {Junyoung Chung and {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and KyungHyun Cho and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1412.3555},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChungGCB14},
  Comment                  = {http://arxiv.org/abs/1412.3555},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1412.3555v1.pdf:PDF},
  Review                   = {Comparison between LSTM and GRU.},
  Timestamp                = {Thu, 01 Jan 2015 19:51:08 +0100}
}

@Article{ChuKa2015,
  Title                    = {A Recurrent Latent Variable Model for Sequential Data},
  Author                   = {Junyoung Chung and Kyle Kastner and Laurent Dinh and Kratarth Goel and Aaron C. Courville and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.02216},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChungKDGCB15},
  Comment                  = {http://arxiv.org/abs/1506.02216},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1506.02216v5.pdf:PDF},
  Review                   = {Added latent random variable into hidden state of an RNN -> VRNN Recurrent version of an Variational Auto Encoder, models the dependencies between latent random variables across subsequent timesteps}
}

@Book{Clerc2006,
  Title                    = {Particle swarm optimization},
  Author                   = {Clerc, Maurice},
  Publisher                = {London [u.a.] : ISTE},
  Year                     = {2006},

  ISBN                     = {978-1-905209-04-0, 1-905209-04-5},
  Key                      = {000971862},
  Keywords                 = {Optimierung, Schwarmintelligenz, Swarm intelligence, Particles (Nuclear physics), Mathematical optimization},
  Review                   = {Particle Swarm Opt. = Stochastic global optimization (backprop is local). No Selection as in genetic algorithms. Number of informants = 3 is the wisest choice wrt number of evaluations. Parallel calculation of displacements a priori desirable as the evaluation of a position is long and difficult for mawk. On the other hand, it rather requires more evaluations to reach the solution (In mawk, there is no single solution).}
}

@InProceedings{DoKo2014,
  Title                    = {Fast and Robust Training of Recurrent Neural Networks for Offline Handwriting Recognition},
  Author                   = {Doetsch, P. and Kozielski, M. and Ney, H.},
  Booktitle                = {Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  Year                     = {2014},
  Month                    = {9},
  Pages                    = {279-284},

  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\06981033.pdf:PDF},
  Keywords                 = {handwriting recognition;learning (artificial intelligence);recurrent neural nets;English handwriting;French handwriting;GPU based implementation;gating units;long short-term memory recurrent neural networks;mini-batch training;modified topology;offline handwriting recognition;recurrent neural networks training;sequence chunking approach;sequence level;squashing functions;Databases;Graphics processing units;Handwriting recognition;Hidden Markov models;Logic gates;Recurrent neural networks;Training;GPU;batch-training;handwriting recognition;recurrent neural networks},
  Review                   = {Newer successful use of LSTMs in handwriting recognition Shows how to batch sequence chunks to speed up training runtime by 3}
}

@Article{Domingo2012,
  Title                    = {A few useful things to know about machine learning},
  Author                   = {Domingos, Pedro},
  Journal                  = {Communications of the ACM},
  Year                     = {2012},
  Number                   = {10},
  Pages                    = {78--87},
  Volume                   = {55},

  Publisher                = {ACM}
}

@Article{DoHe2014,
  Title                    = {Long-term Recurrent Convolutional Networks for Visual Recognition and Description},
  Author                   = {Jeff Donahue and Lisa Anne Hendricks and Sergio Guadarrama and Marcus Rohrbach and Subhashini Venugopalan and Kate Saenko and Trevor Darrell},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1411.4389},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/DonahueHGRVSD14},
  Comment                  = {http://arxiv.org/abs/1411.4389},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1411.4389v3.pdf:PDF},
  Review                   = {successful use of LSTMs in analysis of video data},
  Timestamp                = {Mon, 01 Dec 2014 14:32:13 +0100}
}

@Article{DuHa2011,
  Title                    = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  Author                   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2011},

  Month                    = jul,
  Pages                    = {2121--2159},
  Volume                   = {12},

  Acmid                    = {2021068},
  Comment                  = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\p2121-duchi.pdf:PDF},
  Issue_date               = {2/1/2011},
  Numpages                 = {39},
  Publisher                = {JMLR.org},
  Review                   = {introduces AdaGrad (SGD variant)}
}

@Article{EsMo2009,
  Title                    = {Particle swarm model selection},
  Author                   = {Escalante, Hugo Jair and Montes, Manuel and Sucar, Luis Enrique},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2009},
  Pages                    = {405--440},
  Volume                   = {10},

  Comment                  = {http://www.jmlr.org/papers/volume10/escalante09a/escalante09a.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Systemidentification/escalante09a.pdf:PDF},
  Publisher                = {JMLR. org},
  Review                   = {They show PSMS is reasonable on the example of binary classification problems. Brief description of difference PSO<->Evol. Algorithms. Suggestion for treatment of hyperparameters which only occur for certain other hyperparameters. They recommend k-fold CV, whereas we use only leave one out for one repetition. While doin PSO, track the considered hyperparameters to plot normalized frequency of preferred parameters (is done automatically in config files).}
}

@Article{Gers2001,
  Title                    = {Long short-term memory in recurrent neural networks},
  Author                   = {Gers, Felix},
  Journal                  = {Unpublished PhD dissertation, {\'E}cole Polytechnique F{\'e}d{\'e}rale de Lausanne, Lausanne, Switzerland},
  Year                     = {2001},

  Comment                  = {http://www.felixgers.de/papers/phd.pdf},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\Gersphd.pdf:PDF},
  Publisher                = {Citeseer},
  Review                   = {Summary of LSTMs with peepholes and forget gates. Contains forward and backward pass equations. Contains pseudocode for LSTM w/ peepholes. Has good comparisions of different RNN architectures (TDNN, Elman and even other than RNN)}
}

@InProceedings{GeSch2000,
  Title                    = {Recurrent nets that time and count},
  Author                   = {Gers, F.A. and Schmidhuber, J.},
  Booktitle                = {Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on},
  Year                     = {2000},
  Pages                    = {189-194 vol.3},
  Volume                   = {3},

  Abstract                 = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by “peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes separated by either 50 or 49 discrete time steps, without the help of any short training exemplars. Without external resets or teacher forcing or loss of performance on tasks reported earlier, our LSTM variant also learns to generate very stable sequences of highly nonlinear, precisely timed spikes. This makes LSTM a promising approach for real-world tasks that require to time and count},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/00861302.pdf:PDF},
  Keywords                 = {counting circuits;learning (artificial intelligence);recurrent neural nets;sequences;timing;LSTM;RNN;counting;discrete time steps;highly-nonlinear precisely-timed spike sequences;internal cells;long short-term memory;motor control;multiplicative gates;peephole connections;recurrent neural networks;rhythm detection;sequential tasks;stable sequences;time intervals;timing;Delay;Event detection;Hidden Markov models;Humans;Motor drives;Pattern recognition;Performance loss;Recurrent neural networks;Rhythm;World Wide Web},
  Review                   = {LSTMs with peepholes introduced. Output squashing function h() is useless. Peepholes pipe cell-state to the three gates (unidirectional).}
}

@InProceedings{GeSch1999,
  Title                    = {Learning to forget: continual prediction with LSTM},
  Author                   = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
  Booktitle                = {Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on (Conf. Publ. No. 470)},
  Year                     = {1999},
  Pages                    = {850-855 vol.2},
  Volume                   = {2},

  Abstract                 = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\00818041.pdf:PDF},
  Keywords                 = {recurrent neural nets;adaptive forget gate;learning;long short-term memory;recurrent neural networks;resource allocation},
  Review                   = {Introduces Forget Gates for LSTMs. LSTM without forget gate needs time marker in input sequence in order to know when to consider an input from the past. -> big weakness LSTMs learn to decompose continual sequenced data into subproblems. Different LSTM cell blocks adapt to different time lag dependencies.}
}

@InProceedings{GloBe2010,
  Title                    = {Understanding the difficulty of training deep feedforward neural networks},
  Author                   = {Glorot, Xavier and Bengio, Yoshua},
  Booktitle                = {International conference on artificial intelligence and statistics},
  Year                     = {2010},
  Pages                    = {249--256},

  Comment                  = {http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/glorot10a.pdf:PDF},
  Review                   = {Introduces normalized initialization. That is, for weight inits draw from +-sqrt(6/(fan-in + fan-out)) uniformly. Because of the multiplicative effect through layers, this factor rather maintains "activation variances and backpropagated gradients variance as one moves up or down the network"}
}

@InProceedings{GoSa1986,
  Title                    = {Engineering optimization via genetic algorithm},
  Author                   = {Goldberg, David E and Samtani, Manohar P},
  Booktitle                = {Electronic Computation (1986)},
  Year                     = {1986},
  Organization             = {ASCE},
  Pages                    = {471--482},

  Review                   = {Optimization via Genetic Algorithms}
}

@Unpublished{GoBe2016,
  Title                    = {Deep Learning},
  Author                   = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  Note                     = {Book in preparation for MIT Press},
  Year                     = {2016},

  Url                      = {http://www.deeplearningbook.org}
}

@Article{GraFe2007,
  Title                    = {Multi-Dimensional Recurrent Neural Networks},
  Author                   = {Alex Graves and Santiago Fern{\'{a}}ndez and J{\"{u}}rgen Schmidhuber},
  Journal                  = {CoRR},
  Year                     = {2007},
  Volume                   = {abs/0705.2011},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-0705-2011},
  Comment                  = {http://arxiv.org/abs/0705.2011},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/0705.2011.pdf:PDF},
  Review                   = {MultiDimensional RNN's introduced for multi-dimensional application to vision, video processing etc. Scaling invariant. Für MA nicht relevant, da wir eindimensionale Sequenzen haben. Skimmed only},
  Timestamp                = {Mon, 05 Dec 2011 18:05:34 +0100}
}

@Article{GraLi2009,
  Title                    = {A Novel Connectionist System for Unconstrained Handwriting Recognition},
  Author                   = {Graves, A. and Liwicki, M. and Fernandez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2009},

  Month                    = may,
  Number                   = {5},
  Pages                    = {855-868},
  Volume                   = {31},

  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\04531750.pdf:PDF},
  Keywords                 = {handwriting recognition;handwritten character recognition;hidden Markov models;image segmentation;recurrent neural nets;connectionist system;hidden Markov models;language modeling;overlapping character segmentation;recurrent neural network;unconstrained handwriting databases;unconstrained handwriting text recognition;Connectionist temporal classification;Handwriting recognition;Long Short-Term Memory;Offline handwriting recognition;Online handwriting recognition;Recurrent neural networks;Unconstrained handwriting recognition;bidirectional long short-term memory;connectionist temporal classification;hidden Markov model.;offline handwriting;online handwriting;recurrent neural networks;Algorithms;Automatic Data Processing;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Models, Statistical;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique},
  Review                   = {successful use of LSTMs (and CTC) in handwriting recognition.}
}

@InProceedings{GraSchmi2005,
  Title                    = {Framewise phoneme classification with bidirectional LSTM networks other neural network architectures},
  Author                   = {Graves, A. and Schmidhuber, J.},
  Booktitle                = {Neural Networks, 2005. IJCNN '05. Proceedings. 2005 IEEE International Joint Conference on},
  Year                     = {2005},
  Month                    = jul,
  Pages                    = {602-610 Issues 5-6},
  Volume                   = {18},

  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1-s2.0-S0893608005001206-main.pdf:PDF},
  Keywords                 = {recurrent neural nets;signal classification;speech recognition;bidirectional training;continuous speech recognition;framewise phoneme classification;long short term memory network;recurrent neural networks;speech database;Acoustic measurements;Data analysis;Databases;Electronic mail;Error correction;Hidden Markov models;Memory architecture;Neural networks;Recurrent neural networks;Speech recognition},
  Review                   = {BLSTMs introduced. Has PseudoCode for Full Gradient Training instead of a truncated version. LSTM trains much faster than RNN. For MA not suitable because bidirectional looks into future. skimmed only.}
}

@Article{GraWa2014,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and Greg Wayne and Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1410.5401},

  Abstract                 = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Comment                  = {http://arxiv.org/abs/1410.5401},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1410.5401.pdf:PDF},
  Review                   = {NTM introduced. Requires additional memory bank.},
  Timestamp                = {Sun, 02 Nov 2014 11:25:59 +0100}
}

@Article{GreSri2015,
  Title                    = {{LSTM:} {A} Search Space Odyssey},
  Author                   = {Klaus Greff and Rupesh Kumar Srivastava and Jan Koutnik and Bas R. Steunebrink and Juergen Schmidhuber},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1503.04069},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GreffSKSS15},
  Comment                  = {http://arxiv.org/abs/1503.04069},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1503.04069v1.pdf:PDF},
  Review                   = {Compares LSTM modifications. Vanilla LSTMs are decent enough even without peepholes or update gate which strongly simplifies them. Forget gate and output activation function are most important. Learning rate is most crucial hyperparameter (>66%) for error rate. Momentum is surprisingly unimportant. Hyperparameter interaction is relatively weak, so tweaking independently is possible.},
  Timestamp                = {Thu, 09 Apr 2015 11:33:20 +0200}
}

@Article{GreDa2015,
  Title                    = {{DRAW:} {A} Recurrent Neural Network For Image Generation},
  Author                   = {Karol Gregor and Ivo Danihelka and Alex Graves and Daan Wierstra},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1502.04623},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GregorDGW15},
  Comment                  = {http://arxiv.org/abs/1502.04623},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1502.04623.pdf:PDF},
  Review                   = {sucessful use in image generation},
  Timestamp                = {Mon, 02 Mar 2015 14:17:34 +0100}
}

@Unpublished{HiNi2012,
  Title                    = {Lecture 6e rmsprop: Divide the gradient by a running average of its recent magnitude},
  Author                   = {Geoffrey Hinton and Nitish Srivastava and Kevin Swersky},
  Note                     = {Lecture slides, published online. https://www.youtube.com/watch?v=LGA-gRkLEsI},
  Year                     = {2012},

  Owner                    = {Wilhelm},
  Review                   = {Introduces RMSprop},
  Timestamp                = {2016.01.06}
}

@Article{HoSch1997,
  Title                    = {Long Short-Term Memory},
  Author                   = {Hochreiter, S and Schmidhuber, J},
  Journal                  = {Neural Computation},
  Year                     = {1997},

  Month                    = nov,
  Number                   = {8},
  Pages                    = {1735-1780},
  Volume                   = {9},

  Abstract                 = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  Comment                  = {http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/LSTMFirstIntroduction.pdf:PDF},
  Review                   = {Introduces LSTM. No forget gates yet. Handles noise, distributed representations and continuous values. Generalizes well. Has comparisons of some older architectures.}
}

@Article{HoSti1989,
  Title                    = {Multilayer feedforward networks are universal approximators},
  Author                   = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  Journal                  = {Neural networks},
  Year                     = {1989},
  Number                   = {5},
  Pages                    = {359--366},
  Volume                   = {2},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/Kornick_et_al.pdf:PDF},
  Publisher                = {Elsevier},
  Review                   = {mathematical prove that mlp approximates any function arbitrarily precise}
}

@InProceedings{HuHo2014,
  Title                    = {An efficient approach for assessing hyperparameter importance},
  Author                   = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  Booktitle                = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  Year                     = {2014},
  Pages                    = {754--762},

  Comment                  = {http://jmlr.org/proceedings/papers/v32/hutter14.html},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/hutter14.pdf:PDF},
  Review                   = {Methods for gaining knowledge about hyperparameter importance}
}

@InCollection{HuHo2011,
  Title                    = {Sequential model-based optimization for general algorithm configuration},
  Author                   = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  Booktitle                = {Learning and Intelligent Optimization},
  Publisher                = {Springer},
  Year                     = {2011},
  Pages                    = {507--523},

  Review                   = {About Bayesian Optimization (Hyperopt in python)}
}

@InProceedings{JoZa2015,
  Title                    = {An Empirical Exploration of Recurrent Network Architectures},
  Author                   = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  Booktitle                = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  Year                     = {2015},
  Pages                    = {2342--2350},

  Comment                  = {http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/jozefowicz15.pdf:PDF},
  Review                   = {Finding an RNN architecture outperforming LSTM or GRU (they failed) Adding bias to the forget gate leverages performance. It's hard to find an architecture better than LSTM or GRU. Forget gate is the crucial component in LSTM.}
}

@Article{SoWi2014,
  Title                    = {{Protein Secondary Structure Prediction with Long Short Term Memory Networks}},
  Author                   = {{Kaae S{\o}nderby}, S. and {Winther}, O.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2014},

  Month                    = dec,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2014arXiv1412.7828K},
  Archiveprefix            = {arXiv},
  Comment                  = {http://arxiv.org/abs/1412.7828},
  Eprint                   = {1412.7828},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1412.7828v2.pdf:PDF},
  Keywords                 = {Quantitative Biology - Quantitative Methods, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
  Primaryclass             = {q-bio.QM},
  Review                   = {successful use of LSTMs in protein secondary structure prediction}
}

@Article{KaDa2015,
  Title                    = {Grid Long Short-Term Memory},
  Author                   = {Nal Kalchbrenner and Ivo Danihelka and Alex Graves},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1507.01526},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KalchbrennerDG15},
  Comment                  = {http://arxiv.org/abs/1507.01526},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1507.01526v1.pdf:PDF},
  Review                   = {RNN Future: Grid LSTMs},
  Timestamp                = {Sun, 02 Aug 2015 18:42:02 +0200}
}

@InProceedings{KeEb1995,
  Title                    = {Particle swarm optimization},
  Author                   = {J. Kennedy and R. Eberhart},
  Booktitle                = {Neural Networks, 1995. Proceedings., IEEE International Conference on},
  Year                     = {1995},
  Month                    = nov,
  Pages                    = {1942-1948 vol.4},
  Volume                   = {4},

  Keywords                 = {artificial intelligence;genetic algorithms;neural nets;search problems;simulation;artificial life;evolution;genetic algorithms;multidimensional search;neural network;nonlinear functions;optimization;particle swarm;simulation;social metaphor;Artificial neural networks;Birds;Educational institutions;Genetic algorithms;Humans;Marine animals;Optimization methods;Particle swarm optimization;Performance evaluation;Testing},
  Review                   = {introduces PSO originally}
}

@Article{KeKa2003,
  Title                    = {On the need for time series data mining benchmarks: a survey and empirical demonstration},
  Author                   = {Keogh, Eamonn and Kasetty, Shruti},
  Journal                  = {Data Mining and knowledge discovery},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {349--371},
  Volume                   = {7},

  Comment                  = {http://www.cs.ucr.edu/~eamonn/sigkdd_bench.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Systemidentification/sigkdd_bench.pdf:PDF},
  Publisher                = {Springer},
  Review                   = {Big Comparison of time series similarity search techniques.}
}

@InProceedings{KeLi2005,
  Title                    = {HOT SAX: efficiently finding the most unusual time series subsequence},
  Author                   = {E. Keogh and J. Lin and A. Fu},
  Booktitle                = {Data Mining, Fifth IEEE International Conference on},
  Year                     = {2005},
  Month                    = {Nov},
  Pages                    = {8 pp.-},

  Doi                      = {10.1109/ICDM.2005.79},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Systemidentification/01565683.pdf:PDF},
  ISSN                     = {1550-4786},
  Keywords                 = {data mining;time series;HOT SAX;anomaly detection;clustering quality;data cleaning;data mining;summarization;time series discords;Aerospace industry;Cleaning;Computer science;Data mining;Detection algorithms;Detectors;Monitoring;Space shuttles;Surveillance;Telemetry;Anomaly Detection;Clustering;Time Series Data Mining},
  Review                   = {Finding Time Series Discords.}
}

@Article{KiBa2014,
  Title                    = {Adam: {A} Method for Stochastic Optimization},
  Author                   = {Diederik P. Kingma and Jimmy Ba},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1412.6980},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14},
  Comment                  = {http://arxiv.org/abs/1412.6980},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1412.6980v8.pdf:PDF},
  Review                   = {Introduces Adam. Combines RMSprop and AdaGrad. Adam better suited for sparse gradients than rmsprop. Parameter updates dependent on moving average of 1st and 2nd stochastic moment of the gradient.},
  Timestamp                = {Thu, 01 Jan 2015 19:51:08 +0100}
}

@Article{Kirkpatrick1984,
  Title                    = {Optimization by simulated annealing: Quantitative studies},
  Author                   = {Kirkpatrick, Scott},
  Journal                  = {Journal of statistical physics},
  Year                     = {1984},
  Number                   = {5-6},
  Pages                    = {975--986},
  Volume                   = {34},

  Publisher                = {Springer}
}

@InProceedings{Kohavi1995,
  Title                    = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
  Author                   = {Ron Kohavi},
  Booktitle                = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence},
  Year                     = {1995},
  Pages                    = {1137--1143},
  Publisher                = {Morgan Kaufmann},

  Comment                  = {http://ai.stanford.edu/~ronnyk/accEst.pdf},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\Systemidentification\\crossvalidation_bootstrap1995.pdf:PDF},
  Review                   = {cross validation vs. bootstrapping. stratification is generally better. LOOCV is unbiased but high variance | bootstrap is highly biased but with low variance. stratified 10-fold cross validation is recommended for model selection.}
}

@Article{KoGre2014,
  Title                    = {A Clockwork {RNN}},
  Author                   = {Jan Koutnik and Klaus Greff and Faustino J. Gomez and Juergen Schmidhuber},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1402.3511},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KoutnikGGS14},
  Comment                  = {http://arxiv.org/abs/1402.3511},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1402.3511v1.pdf:PDF},
  Review                   = {Extension to Simple RNNs. Hidden layer is separated in modules, each module is processed only if their time comes. Each module has its time period as in a clockwork. Fast clocking modules represent high frequency dependencies, low clocking ones rep. long-term deps. Simpler to train, as backpropagating through time only effects the processed modules during that certain time step.},
  Timestamp                = {Wed, 05 Mar 2014 14:43:44 +0100}
}

@InProceedings{KriSu2012,
  Title                    = {Imagenet classification with deep convolutional neural networks},
  Author                   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2012},
  Pages                    = {1097--1105},

  Comment                  = {http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/imagenet.pdf:PDF},
  Review                   = {Introduces ReLU Neurons and DropOut. They use Conv Nets.}
}

@Book{Kylander1995,
  Title                    = {Thermal modelling of small cage induction motors},
  Author                   = {Kylander, Gunnar},
  Publisher                = {Chalmers University of Technology},
  Year                     = {1995},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/Diss_Kylander.pdf:PDF}
}

@Article{LaWa1990,
  Title                    = {A time-delay neural network architecture for isolated word recognition},
  Author                   = {Lang, Kevin J and Waibel, Alex H and Hinton, Geoffrey E},
  Journal                  = {Neural networks},
  Year                     = {1990},
  Number                   = {1},
  Pages                    = {23--43},
  Volume                   = {3},

  Comment                  = {http://www.cs.toronto.edu/~hinton/absps/langTDNN.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/langTDNN.pdf:PDF},
  Publisher                = {Elsevier},
  Review                   = {Introduces TDNN. Skimmed only.}
}

@InProceedings{LeBo1998,
  Title                    = {Efficient BackProp},
  Author                   = {LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  Booktitle                = {Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop},
  Year                     = {1998},
  Organization             = {Springer-Verlag},
  Pages                    = {9--50},

  Comment                  = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/lecun-98b.pdf:PDF},
  Review                   = {About efficient Backprop (lots of tips, also to initialization and shuffling examples etc.)}
}

@Article{LeVa2014,
  Title                    = {Optimal Control of Traction Motor Drives Under Electrothermal Constraints},
  Author                   = {J. Lemmens and P. Vanassche and J. Driesen},
  Journal                  = {IEEE Journal of Emerging and Selected Topics in Power Electronics},
  Year                     = {2014},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {249-263},
  Volume                   = {2},

  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/phd_joris_lemmens_final.pdf:PDF},
  Keywords                 = {electric current control;frequency control;machine vector control;permanent magnet motors;reliability;synchronous motor drives;thermal management (packaging);torque control;traction motor drives;PMSM drives;active thermal management;control strategy;current control limit;drivetrain;driving cycle test;excessive temperature cycling;failure mechanisms;maximum acceleration test;maximum standstill torque test;motor temperatures;optimal dq-current control vectors;peak torque;permanent magnet synchronous motor drives;power density requirements;real-time estimation;reliability;representative vehicle loads;speed-torque envelope;switching device feedback;switching frequency regulation;temperature constraints;traction motor drives;voltage constraints;Insulated gate bipolar transistors;Inverters;Iron;Loss measurement;Torque;Voltage control;Voltage measurement;Efficiency optimization;PMSM control;Power electronics;efficiency optimization;permanent magnet synchronous motor (PMSM) control;power electronics;reliability;thermal management}
}

@InProceedings{LiHu2015,
  Title                    = {Recurrent convolutional neural network for object recognition},
  Author                   = {Ming Liang and Xiaolin Hu},
  Booktitle                = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2015},
  Month                    = jun,
  Pages                    = {3367-3375},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf:PDF},
  Keywords                 = {computer vision;feedforward neural nets;learning (artificial intelligence);object recognition;recurrent neural nets;CIFAR-10;CIFAR-100;MNIST;RCNN;SVHN;arbitrarily deep network;brain;computer vision task;convolutional neural network;feed-forward architecture;object recognition;recurrent CNN;recurrent convolutional neural network;Computational modeling;Convolution;Feeds},
  Review                   = {successful use in object recognition}
}

@Article{LiBi1996,
  Title                    = {Learning long-term dependencies in NARX recurrent neural networks},
  Author                   = {Lin, Tsungnam and Horne, Bil G and Ti{\v{n}}o, Peter and Giles, C Lee},
  Journal                  = {Neural Networks, IEEE Transactions on},
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {1329--1338},
  Volume                   = {7},

  Comment                  = {http://deeplearning.cs.cmu.edu/pdfs/Narx.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/Narx.pdf:PDF},
  Publisher                = {IEEE},
  Review                   = {Introduces NARX. Lessens vanishing gradient problem but doesn't solve it. Skimmed only.}
}

@Article{Lipton2015,
  Title                    = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  Author                   = {Zachary Chase Lipton},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.00019},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/Lipton15},
  Comment                  = {http://arxiv.org/abs/1506.00019},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1506.00019.pdf:PDF},
  Review                   = {Has elaborated a homogenous notation for RNNs. Stick to this paper for notation! FNNs assume data to be independent, thats why they fail time series prediction.},
  Timestamp                = {Wed, 01 Jul 2015 15:10:24 +0200}
}

@Article{LipZac2015,
  Title                    = {Learning to Diagnose with LSTM Recurrent Neural Networks},
  Author                   = {Lipton, Zachary C and Kale, David C and Elkan, Charles and Wetzell, Randall},
  Journal                  = {arXiv preprint arXiv:1511.03677},
  Year                     = {2015},

  Comment                  = {http://arxiv.org/abs/1511.03677},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1511.03677v6.pdf:PDF},
  Review                   = {LSTM medical diagnosis classification on time series data. Regularization: dropout, auxiliary funcs, target replication (linear gain) Regularization important for small data sets.}
}

@Article{LuSu2014,
  Title                    = {Addressing the Rare Word Problem in Neural Machine Translation},
  Author                   = {Thang Luong and Ilya Sutskever and Quoc V. Le and Oriol Vinyals and Wojciech Zaremba},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1410.8206},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/LuongSLVZ14},
  Comment                  = {http://arxiv.org/abs/1410.8206},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1410.8206v4.pdf:PDF},
  Review                   = {successful use of LSTMs on Machine Translation},
  Timestamp                = {Sun, 02 Nov 2014 11:25:59 +0100}
}

@Article{MaXu2014,
  Title                    = {Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)},
  Author                   = {Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Alan L. Yuille},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1412.6632},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/MaoXYWY14a},
  Comment                  = {http://arxiv.org/abs/1412.6632},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1412.6632v5.pdf:PDF},
  Review                   = {successful use for image captioning},
  Timestamp                = {Thu, 01 Jan 2015 19:51:08 +0100}
}

@InProceedings{MaFe2014,
  Title                    = {Multi-resolution linear prediction based features for audio onset detection with bidirectional LSTM neural networks},
  Author                   = {Marchi, E. and Ferroni, G. and Eyben, F. and Gabrielli, L. and Squartini, S. and Schuller, B.},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  Year                     = {2014},
  Month                    = may,
  Pages                    = {2164-2168},

  Abstract                 = {A plethora of different onset detection methods have been proposed in the recent years. However, few attempts have been made with respect to widely-applicable approaches in order to achieve superior performances over different types of music and with considerable temporal precision. In this paper, we present a multi-resolution approach based on discrete wavelet transform and linear prediction filtering that improves time resolution and performance of onset detection in different musical scenarios. In our approach, wavelet coefficients and forward prediction errors are combined with auditory spectral features and then processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. We compare results with state-of-the-art methods on a dataset that includes Bello, Glover and ISMIR 2004 Ballroom sets, and we conclude that our approach significantly outperforms existing methods in terms of F-Measure. For pitched non percussive music an absolute improvement of 7.5% is reported.},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\06853982.pdf:PDF},
  Keywords                 = {audio signal processing;discrete wavelet transforms;filtering theory;prediction theory;recurrent neural nets;signal detection;signal resolution;audio onset detection methods;auditory spectral features;bidirectional LSTM neural networks;bidirectional long short-term memory recurrent neural network;discrete wavelet transform;forward prediction errors;linear prediction filtering;multi-resolution linear prediction based features;non percussive music;reduction function;time resolution;wavelet coefficients;Conferences;Discrete wavelet transforms;Feature extraction;Neural networks;Speech;Speech processing;Audio Onset Detection;Bidirectional LongShort Term Memory;Discrete Wavelet Transform;Linear Prediction;Neural Networks},
  Review                   = {successful use of LSTMs in audio analysis}
}

@InProceedings{Martens2010,
  Title                    = {Deep learning via Hessian-free optimization},
  Author                   = {Martens, James},
  Booktitle                = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  Year                     = {2010},
  Pages                    = {735--742},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/458.pdf:PDF}
}

@Article{MeRo1991,
  Title                    = {Lumped parameter thermal model for electrical machines of TEFC design},
  Author                   = {Mellor, PH and Roberts, D and Turner, DR},
  Journal                  = {Electric Power Applications, IEE Proceedings B},
  Year                     = {1991},
  Number                   = {5},
  Pages                    = {205--218},
  Volume                   = {138},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/MRT91.pdf:PDF},
  Publisher                = {IET}
}

@Article{Monner201270,
  Title                    = {A generalized LSTM-like training algorithm for second-order recurrent neural networks },
  Author                   = {Derek Monner and James A. Reggia},
  Journal                  = {Neural Networks },
  Year                     = {2012},
  Pages                    = {70 - 83},
  Volume                   = {25},

  __markedentry            = {[Wilhelm:1]},
  Abstract                 = {The Long Short Term Memory (LSTM) is a second-order recurrent neural network architecture that excels at storing sequential short-term memories and retrieving them many time-steps later. LSTM’s original training algorithm provides the important properties of spatial and temporal locality, which are missing from other training approaches, at the cost of limiting its applicability to a small set of network architectures. Here we introduce the Generalized Long Short-Term Memory(LSTM-g) training algorithm, which provides LSTM-like locality while being applicable without modification to a much wider range of second-order network architectures. With LSTM-g, all units have an identical set of operating instructions for both activation and learning, subject only to the configuration of their local environment in the network; this is in contrast to the original \{LSTM\} training algorithm, where each type of unit has its own activation and training instructions. When applied to \{LSTM\} architectures with peephole connections, LSTM-g takes advantage of an additional source of back-propagated error which can enable better performance than the original algorithm. Enabled by the broad architectural applicability of LSTM-g, we demonstrate that training recurrent networks engineered for specific tasks can produce better results than single-layer networks. We conclude that LSTM-g has the potential to both improve the performance and broaden the applicability of spatially and temporally local gradient-based training algorithms for recurrent neural networks.},
  Comment                  = {http://www.sciencedirect.com/science/article/pii/S0893608011002036},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/lstm_g_2012.pdf:PDF},
  Keywords                 = {Recurrent neural network},
  Review                   = {Introduces LSTM-g. An reinterpreted version of LSTM. LSTM-g utilizes a source of error that LSTM neglects: error responsibilities back-propagated from the output gates across the peephole connections to the associated memory cells, and beyond}
}

@Article{MoHa1995,
  Title                    = {A simple weight decay can improve generalization},
  Author                   = {J. Moody and S. Hanson and A. Krogh and J. A. Hertz},
  Journal                  = {Advances in Neural Information Processing Systems},
  Year                     = {1995}
}

@Article{NeVi2015,
  Title                    = {Adding Gradient Noise Improves Learning for Very Deep Networks},
  Author                   = {Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  Journal                  = {arXiv preprint arXiv:1511.06807},
  Year                     = {2015},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1511.06807.pdf:PDF},
  Review                   = {introduces adding gradient noise gauss distributed}
}

@Article{Nesterov1983,
  Title                    = {A method of solving a convex programming problem with convergence rate O(1/sqr(k))},
  Author                   = {Yurii Nesterov},
  Journal                  = {Soviet Mathematics Doklady},
  Year                     = {1983},
  Pages                    = {372-376},
  Volume                   = {27},

  Review                   = {introduces Nesterov momentum training}
}

@InProceedings{ng1997preventing,
  Title                    = {Preventing "overfitting" of cross-validation data},
  Author                   = {Ng, Andrew Y},
  Booktitle                = {ICML},
  Year                     = {1997},
  Pages                    = {245--253},
  Volume                   = {97},

  Comment                  = {http://ai.stanford.edu/~ang/papers/cv-final.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Systemidentification/ngPreventOverfitting.pdf:PDF},
  Review                   = {Hypotheses with lower CV error do not necessarily have lower expected generalization error. Hypotheses with lower generalization errors do have lower expected CV errors, but may have a smaller chance of having extremely low CV errors because their distributions of CV errors are less spread-out than those of hypotheses with higher generalizations errors. LOOCVCV looses to best-of-n if CV set is unnoised. On NNs it also looses, empirically.}
}

@Article{PaMi2012,
  Title                    = {Understanding the exploding gradient problem},
  Author                   = {Razvan Pascanu and Tomas Mikolov and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1211.5063},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1211-5063},
  Comment                  = {http://arxiv.org/abs/1211.5063},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1211.5063v2.pdf:PDF},
  Review                   = {Introduces Gradient Clipping (as scaling every element). Good Intro to RNNs (has refs to its introduction). Mathematical explanation of gradient explosion (has refs to this' introduction). Explains exploding gradient with dynamical systems theory (Attractor basins in state space etc).},
  Timestamp                = {Sat, 01 Dec 2012 20:32:38 +0100}
}

@PhdThesis{Roberts1986,
  Title                    = {The application of an Induction Motor thermal model to motor protection and other functions},
  Author                   = {David Roberts},
  School                   = {University of Liverpool},
  Year                     = {1986},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/Diss_Roberts.pdf:PDF},
  Review                   = {It says "Internal radial heat transfer is neglible!"}
}

@InProceedings{SakHa2014,
  Title                    = {Long short-term memory recurrent neural network architectures for large scale acoustic modeling},
  Author                   = {Sak, Hasim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  Booktitle                = {Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH)},
  Year                     = {2014},

  Comment                  = {http://193.6.4.39/~czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141304.PDF},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\IS141304.PDF:PDF},
  Review                   = {successful use of LSTMs in large scale acoustic modeling}
}

@Article{SaSe2015,
  Title                    = {Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition},
  Author                   = {Hasim Sak and Andrew W. Senior and Kanishka Rao and Fran{\c{c}}oise Beaufays},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1507.06947},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/SakSRB15},
  Comment                  = {http://arxiv.org/abs/1507.06947},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1507.06947v1.pdf:PDF},
  Review                   = {Successful use in ASR},
  Timestamp                = {Sun, 02 Aug 2015 18:42:02 +0200}
}

@Book{Schroeder2010,
  Title                    = {Intelligente Verfahren : Identifikation und Regelung nichtlinearer Systeme},
  Author                   = {Schr{\"o}der, Dierk},
  Publisher                = {Springer},
  Year                     = {2010},

  Address                  = {Berlin ; Heidelberg},

  Comment                  = {https://katalog.ub.uni-paderborn.de/records/PAD_ALEPH001354229},
  File                     = {:Dokumente/Masterarbeit/Literatur/Systemidentification/Systemidentifikation_IntelligenteVerfahren_Schroeder2010.pdf:PDF},
  ISBN                     = {978-3-642-11398-7},
  Keywords                 = {Nichtlineares System, Soft Computing, Lernendes System, Systemidentifikation, Beobachter, Reglerentwurf, Artificial intelligence, Control engineering systems, Engineering},
  Review                   = {Kapitel 6-8 interessant}
}

@Book{Schroeder2007,
  Title                    = {Elektrische Antriebe - Grundlagen},
  Author                   = {Dierk Schr{\"o}der},
  Publisher                = {Springer Vieweg},
  Year                     = {2007},

  Address                  = {Berlin Heidelberg},
  Series                   = {Springer-Lehrbuch},
  Volume                   = {5},

  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/Grundlagen PMSM/Schroeder2007_Elektrische_Antriebe-Grundlagen.pdf:PDF},
  Owner                    = {wilhelmk},
  Timestamp                = {2016.04.21}
}

@Article{SchuPa1997,
  Title                    = {Bidirectional recurrent neural networks},
  Author                   = {Schuster, M. and Paliwal, Kuldip K.},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {1997},

  Month                    = nov,
  Number                   = {11},
  Pages                    = {2673-2681},
  Volume                   = {45},

  __markedentry            = {[Wilhelm:]},
  Abstract                 = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\00650093.pdf:PDF},
  Keywords                 = {learning by example;pattern classification;recurrent neural nets;speech processing;speech recognition;statistical analysis;TIMIT database;artificial data;bidirectional recurrent neural networks;classification experiments;complete symbol sequences;conditional posterior probability;learning from examples;negative time direction;phonemes;positive time direction;real data;regression experiments;regular recurrent neural network;speech recognition;training;Artificial neural networks;Control systems;Databases;Parameter estimation;Probability;Recurrent neural networks;Shape;Speech recognition;Telecommunication control;Training data},
  Review                   = {Introduces BRNN. Faster and more precise than RNN. Full sequence length required for each prediction. Thus not suitable for MA.}
}

@PhdThesis{Suts2013,
  Title                    = {Training recurrent neural networks},
  Author                   = {Sutskever, Ilya},
  School                   = {University of Toronto},
  Year                     = {2013},

  __markedentry            = {[Wilhelm:1]},
  Comment                  = {http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/ilya_sutskever_phd_thesis.pdf:PDF},
  Review                   = {On different training methods}
}

@Article{SuMa2013,
  Title                    = {On the importance of initialization and momentum in deep learning},
  Author                   = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  Journal                  = {JMLR W\&CP 28 (3)},
  Year                     = {2013},
  Pages                    = {1139–1147},

  __markedentry            = {[Wilhelm:1]},
  Comment                  = {http://jmlr.org/proceedings/papers/v28/sutskever13.html},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/sutskever13.pdf:PDF},
  Owner                    = {Wilhelm},
  Review                   = {HF Optimization beats momentum sgd and nesterov. Initialization of weights should be set according to important inputs (for input-hidden layer). A few other tips for initialization which seem very special and hardly applicable in a general fashion.},
  Timestamp                = {2015.11.05}
}

@Article{TaiSo2015,
  Title                    = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  Author                   = {Kai Sheng Tai and Richard Socher and Christopher D. Manning},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1503.00075},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/TaiSM15},
  Comment                  = {http://arxiv.org/abs/1503.00075},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1503.00075.pdf:PDF},
  Review                   = {Tree structured LSTMs introduced. Evaluated better only on semantic modeling.},
  Timestamp                = {Thu, 09 Apr 2015 11:33:20 +0200}
}

@InProceedings{Chainer2015,
  Title                    = {Chainer: a Next-Generation Open Source Framework for Deep Learning},
  Author                   = {Tokui, Seiya and Oono, Kenta and Hido, Shohei and Clayton, Justin},
  Booktitle                = {Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)},
  Year                     = {2015},

  Comment                  = {http://learningsys.org/papers/LearningSys_2015_paper_33.pdf},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/LearningSys_2015_paper_33.pdf:PDF}
}

@Article{ViFo2015,
  Title                    = {Pointer Networks},
  Author                   = {Oriol Vinyals and Meire Fortunato and Navdeep Jaitly},
  Journal                  = {ArXiv e-prints},
  Year                     = {2015},
  Volume                   = {abs/1506.03134},

  Comment                  = {http://arxiv.org/pdf/1506.03134v1.pdf},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1506.03134.pdf:PDF},
  Owner                    = {Wilhelm},
  Timestamp                = {2015.12.12}
}

@Article{WaBo2016,
  Title                    = {Global Identification of a Low-Order Lumped-Parameter Thermal Network for Permanent Magnet Synchronous Motors},
  Author                   = {O. Wallscheid and J. B{\"o}cker},
  Journal                  = {IEEE Transactions on Energy Conversion},
  Year                     = {2016},

  Month                    = mar,
  Number                   = {1},
  Pages                    = {354-365},
  Volume                   = {31},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/Thermische Modellierung/2015_IEEE_Transaction_Wallscheid.pdf:PDF},
  Keywords                 = {linear parameter varying systems;lumped parameter networks;particle swarm optimisation;permanent magnet motors;stators;synchronous motors;PMSM;global identification;linear parameter-varying systems;low-order lumped-parameter thermal network;particle swarm optimization;permanent magnet synchronous motors;power 60 kW;stator teeth;stator winding;stator yoke;Loss measurement;Permanent magnet motors;Stators;Synchronous motors;Temperature measurement;Traction motors;Windings;Estimation;particle swarm optimization;permanent magnet machines;system identification;temperature dependence;temperature sensors;thermal analysis;thermal management},
  Review                   = {Oliver Wallscheids Analyse von LPTN fuer PMSM.}
}

@Article{Werbos1990,
  Title                    = {Backpropagation through time: what it does and how to do it},
  Author                   = {Werbos, Paul J},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1990},
  Number                   = {10},
  Pages                    = {1550--1560},
  Volume                   = {78},

  Comment                  = {http://deeplearning.cs.cmu.edu/pdfs/Werbos.backprop.pdf},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\BPTT.pdf:PDF},
  Publisher                = {IEEE},
  Review                   = {BPTT explanation. Only for Feedforward NN and time delayed NN though. Claims BPTT is not robust to noise !? Use "adaptive critic" instead.}
}

@Article{WesCho2014,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and Sumit Chopra and Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Comment                  = {http://arxiv.org/abs/1410.3916},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1410.3916v11.pdf:PDF},
  Review                   = {Memory Networks introduced. Picking Answers from a database to respond to questions. Unterschied zu NTM: larger storage, focus on language n reasoning tasks, less complex models. Ungeeignet für MA, da big Datenbank nebenbei gelesen und beschrieben wird. Skimmed only.},
  Timestamp                = {Sun, 02 Nov 2014 11:25:59 +0100}
}

@Article{WiRa2003,
  Title                    = {The general inefficiency of batch training for gradient descent learning},
  Author                   = {Wilson, D Randall and Martinez, Tony R},
  Journal                  = {Neural Networks},
  Year                     = {2003},
  Number                   = {10},
  Pages                    = {1429--1451},
  Volume                   = {16},

  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/Wilson.nn03.batch.pdf:PDF},
  Publisher                = {Elsevier},
  Review                   = {batch training converges much slower than on-line training. But Minibatch training is the best}
}

@Article{WiSte2010,
  Title                    = {Methods of resistance estimation in permanent magnet synchronous motors for real-time thermal management},
  Author                   = {Wilson, Simon Delamere and Stewart, Paul and Taylor, Benjamin P},
  Journal                  = {Energy Conversion, IEEE Transactions on},
  Year                     = {2010},
  Number                   = {3},
  Pages                    = {698--707},
  Volume                   = {25},

  Comment                  = {http://eprints.lincoln.ac.uk/2899/1/Methods.pdf},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\Thermische Modellierung\\Wilson_Stewart_methods_of_resistance_estim.pdf:PDF},
  Publisher                = {IEEE},
  Review                   = {Has nice intro to dq-coordinates and dc current injection Shows two methods of current injection without producing torque}
}

@Article{XuBa2015,
  Title                    = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  Author                   = {Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron C. Courville and Ruslan Salakhutdinov and Richard S. Zemel and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1502.03044},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/XuBKCCSZB15},
  Comment                  = {http://arxiv.org/abs/1502.03044},
  File                     = {:C\:\\Users\\Wilhelm\\Documents\\Uni_PB\\MA\\Literatur\\RNNs\\1502.03044v2.pdf:PDF},
  Review                   = {RNN Future: Attention},
  Timestamp                = {Mon, 02 Mar 2015 14:17:34 +0100}
}

@Article{Yao2015,
  Title                    = {Depth-Gated LSTM},
  Author                   = {Kaisheng Yao and Trevor Cohn and Katerina Vylomova and Kevin Duh and Chris Dyer},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1508.03790},

  __markedentry            = {[wilhelmk:]},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/YaoCVDD15},
  Comment                  = {http://arxiv.org/abs/1508.03790},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1508.03790v4.pdf:PDF},
  Review                   = {LSTM variation},
  Timestamp                = {Tue, 01 Sep 2015 14:42:40 +0200}
}

@InBook{YuWi2011,
  Title                    = {Levenberg-Marquardt Training},
  Author                   = {Hao Yu and B. M. Wilamowski},
  Chapter                  = {12},
  Pages                    = {1-15},
  Publisher                = {CRC Press},
  Year                     = {2011},
  Edition                  = {Industrial Electronics Handbook 2nd Edition},
  Series                   = {Intelligent Systems},
  Volume                   = {5},

  Comment                  = {http://www.eng.auburn.edu/~wilambm/pap/2011/K10149_C012.pdf},
  File                     = {:home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/K10149_C012.pdf:PDF},
  Review                   = {About Levenberg Marquardt Training}
}

@Article{ZaSu2014,
  Title                    = {Recurrent Neural Network Regularization},
  Author                   = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1409.2329},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZarembaSV14},
  Comment                  = {http://arxiv.org/abs/1409.2329},
  File                     = {:/home/wilhelmk/Dokumente/Masterarbeit/Literatur/RNNs/1409.2329v5.pdf:PDF},
  Review                   = {How to regularize RNN with LSTM. Dropout method can be used on RNNs with LSTMs when having the dropout operator on non-recurring connections only. Dropout reduces overfitting , also on LSTMs with many free parameters.},
  Timestamp                = {Wed, 01 Oct 2014 15:00:04 +0200}
}

@Article{Zeiler2012,
  Title                    = {ADADELTA: An Adaptive Learning Rate Method},
  Author                   = {Matthew D. Zeiler},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1212.5701},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-5701},
  Comment                  = {http://arxiv.org/abs/1212.5701},
  File                     = {:Dokumente/Masterarbeit/Literatur/RNNs/1212.5701v1.pdf:PDF},
  Review                   = {introduces AdaDelta},
  Timestamp                = {Wed, 02 Jan 2013 09:49:04 +0100}
}

@Standard{VDE0530,
  Title                    = {DIN EN 60034-1 VDE 0530-1:2011-02 Drehende elektrische Maschinen},
  Institution              = {Deutsches Institut für Normung},
  Organization             = {Verband der Elektrotechnik Elektronik Informationstechnik e.V.}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Reviewed\;0\;BeBe2012\;BeSi1994\;BoCa2009\;Clerc2006\;
GeSch1999\;GeSch2000\;Gers2001\;GloBe2010\;GraFe2007\;GraSchmi2005\;Gr
eSri2015\;HoSch1997\;JoZa2015\;KeLi2005\;KiBa2014\;KoGre2014\;Kohavi19
95\;LaWa1990\;LiBi1996\;Lipton2015\;PaMi2012\;Roberts1986\;SchuPa1997\
;SuMa2013\;VDE0530\;WaBo2016\;Werbos1990\;WesCho2014\;ZaSu2014\;ng1997
preventing\;;
1 ExplicitGroup:cited\;0\;BaRa2015\;BeBou2013\;Bengio2009\;Bishop2006\
;BoCa2009\;Boecker2016\;Chainer2015\;ChoBa2015\;ChuCa2015\;DoHe2014\;D
oKo2014\;Domingo2012\;GeSch1999\;GeSch2000\;GraFe2007\;GraLi2009\;GraS
chmi2005\;GraWa2014\;GreDa2015\;HoSch1997\;HoSti1989\;KoGre2014\;Kylan
der1995\;LeVa2014\;LiBi1996\;LiHu2015\;Lipton2015\;LuSu2014\;MaFe2014\
;MaXu2014\;Martens2010\;MeRo1991\;Roberts1986\;SaSe2015\;SakHa2014\;Sc
hroeder2007\;SchuPa1997\;SoWi2014\;Tensorflow2015\;Theano2012\;VDE0530
\;ViFo2015\;WaBo2016\;WesCho2014\;XuBa2015\;YuWi2011\;;
1 ExplicitGroup:RNN-architectures\;0\;Bengio2009\;ChoBa2015\;ChuCa2015
\;ChuKa2015\;Gers2001\;GraFe2007\;GraSchmi2005\;GraWa2014\;GreSri2015\
;HoSti1989\;JoZa2015\;KaDa2015\;KoGre2014\;KriSu2012\;LaWa1990\;LiBi19
96\;Monner201270\;SchuPa1997\;TaiSo2015\;ViFo2015\;WesCho2014\;Yao2015
\;;
1 ExplicitGroup:LSTM variations\;0\;Bengio2009\;ChoMe2014\;ChuGu2014\;
GeSch2000\;GraSchmi2005\;GreSri2015\;JoZa2015\;KaDa2015\;TaiSo2015\;Ya
o2015\;;
1 ExplicitGroup:Training variations\;0\;BeBe2012\;BeBou2013\;Bengio200
9\;Clerc2006\;DuHa2011\;EsMo2009\;GloBe2010\;HiNi2012\;KiBa2014\;LeBo1
998\;Lipton2015\;Martens2010\;MoHa1995\;Monner201270\;Nesterov1983\;Pa
Mi2012\;SuMa2013\;Suts2013\;WiRa2003\;YuWi2011\;Zeiler2012\;;
1 ExplicitGroup:successful use of LSTMs\;0\;DoHe2014\;DoKo2014\;GraLi2
009\;GreDa2015\;LiHu2015\;LuSu2014\;MaFe2014\;MaXu2014\;SaSe2015\;SakH
a2014\;SoWi2014\;;
1 ExplicitGroup:Thermal management\;0\;BoCa2009\;Kylander1995\;MeRo199
1\;Roberts1986\;Schroeder2007\;VDE0530\;WiSte2010\;;
1 ExplicitGroup:PMSM related\;0\;BoCa2009\;Kylander1995\;LeVa2014\;MeR
o1991\;Roberts1986\;Schroeder2007\;VDE0530\;WaBo2016\;;
1 ExplicitGroup:System Identification\;0\;BoCa2009\;CaTa2010\;KeKa2003
\;KeLi2005\;MeRo1991\;Schroeder2007\;Schroeder2010\;ng1997preventing\;
;
1 ExplicitGroup:Hyperopt\;0\;Clerc2006\;EsMo2009\;GoSa1986\;HuHo2011\;
KeEb1995\;Kirkpatrick1984\;;
}

